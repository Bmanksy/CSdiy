{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add609d2",
   "metadata": {},
   "source": [
    "# Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486aee15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002151A480880>\n",
      "<SparkContext master=local[*] appName=MLlib>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    config(\"spark.executor.memory\", \"4g\").\\\n",
    "    config(\"spark.driver.memory\", \"4g\").\\\n",
    "    config(\"spark.ui.showConsoleProgress\", \"false\").\\\n",
    "    appName(\"MLlib\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c810d39",
   "metadata": {},
   "source": [
    "### 数据读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ba0c2",
   "metadata": {},
   "source": [
    "我们依然使用弹幕数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3916995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/lec14-danmu-144541892.json',\n",
       " 'data/lec14-danmu-144541943.json',\n",
       " 'data/lec14-danmu-160377038.json',\n",
       " 'data/lec14-danmu-148952771.json',\n",
       " 'data/lec14-danmu-150894103.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cids = [144541892, 144541943, 160377038, 148952771, 150894103, 153392221, 156629080, 159982308, 162395026]\n",
    "cids = [144541892, 144541943, 160377038, 148952771, 150894103]\n",
    "jsons = [f\"data/lec14-danmu-{cid}.json\" for cid in cids]\n",
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6d7de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11574000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(jsons, multiLine=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113404ef",
   "metadata": {},
   "source": [
    "出于演示目的，我们随机抽取一小部分数据并缓存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d1bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------+----------+-----------------+----+----------+\n",
      "|      cid|                           content|      date|               id|mode| post_time|\n",
      "+---------+----------------------------------+----------+-----------------+----+----------+\n",
      "|160377038|                            脱水！|2020-01-28|29497785794953219|   1|1580223858|\n",
      "|160377038|                        技术公有化|2020-01-28|29497829588205573|   1|1580217034|\n",
      "|160377038|                              脱水|2020-01-28|29497829597642757|   1|1580217024|\n",
      "|160377038|                              浸泡|2020-01-28|29497833691807747|   1|1580216633|\n",
      "|160377038|                          空位是？|2020-01-28|29497835948343299|   1|1580216368|\n",
      "|160377038|                    你也会空中劈叉|2020-01-28|29497842766184453|   5|1580215732|\n",
      "|160377038|                              脱水|2020-01-29|29497694497013763|   1|1580310440|\n",
      "|160377038|                    让  我  先  跑|2020-01-29|29497730623078403|   5|1580281402|\n",
      "|160377038|                        组织生活会|2020-01-29|29497759658672133|   1|1580255800|\n",
      "|160377038|                          当场揭穿|2020-01-30|29497673583165443|   5|1580376327|\n",
      "|160377038|                                演|2020-01-30|29497677668941829|   1|1580366046|\n",
      "|160377038|                       对 号 入 座|2020-01-30|29497677688864773|   1|1580365701|\n",
      "|160377038|                          狼人自刀|2020-01-30|29497732341170179|   1|1580280897|\n",
      "|160377038|倍啥速，我都是一段段的反复看好几遍|2020-01-31|29497641280733187|   5|1580478569|\n",
      "|160377038|                              浸泡|2020-01-31|29497650110267395|   1|1580445659|\n",
      "+---------+----------------------------------+----------+-----------------+----+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = df.sample(withReplacement=False, fraction=0.001, seed=123)\n",
    "df_small.cache()\n",
    "df_small.show(n=15)\n",
    "df_small.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe487d9",
   "metadata": {},
   "source": [
    "### 数据转换与特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1336c04f",
   "metadata": {},
   "source": [
    "除了 PySpark 自带的变换操作，我们还可以直接编写 Python 函数（基于 Pandas）对数据进行变换，如对弹幕进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4e6a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16808\\197428066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mseg_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdanmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def seg_words(danmu):\n",
    "    # 弹幕通常包含大量空格以醒目，但破坏语义，先移除\n",
    "    danmu = danmu.replace(\" \", \"\")\n",
    "    # 生成一个含有切分后词语的迭代器\n",
    "    cut = jieba.lcut(danmu, cut_all=False)\n",
    "    # 去掉多余的空格\n",
    "    cut = filter(lambda x: x != \" \", cut)\n",
    "    # 再用空格将词语合并\n",
    "    return \" \".join(cut)\n",
    "\n",
    "seg_words_udf = udf(seg_words, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6f51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_seg = df_small.withColumn(\"seg\", seg_words_udf(df_small.content))\n",
    "df_seg.cache()\n",
    "df_seg.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb339718",
   "metadata": {},
   "source": [
    "接下来使用 MLlib 中的 `Tokenizer` 转换器来将词语转为列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87a705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "    \n",
    "tok = Tokenizer(inputCol=\"seg\", outputCol=\"words\")\n",
    "df_tok = tok.transform(df_seg)\n",
    "df_tok.select(\"seg\", \"words\").show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d059d82",
   "metadata": {},
   "source": [
    "移除停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"的\", \"了\", \"是\", \"，\", \"。\", \"？\", \"！\", \"：\", \"（\", \"）\", \"“\", \"”\", \".\", \"…\", \".\"]\n",
    "rmstop = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_words)\n",
    "df_rmstop = rmstop.transform(df_tok)\n",
    "df_rmstop.select(\"words\", \"filtered\").show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b1ab9",
   "metadata": {},
   "source": [
    "进而使用 `CountVectorizer` 来统计词频："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5d640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "# minDF 表示进入字典的词最少需要在多少句子（弹幕）中出现\n",
    "# vocabSize 表示取词频前几位的词语作为字典\n",
    "counter = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\",\n",
    "                          minDF=10, vocabSize=500)\n",
    "\n",
    "counter_model = counter.fit(df_rmstop)\n",
    "print(counter_model.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc84c8",
   "metadata": {},
   "source": [
    "将词频统计器应用到 `DataFrame` 上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafde1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_freq = counter_model.transform(df_rmstop)\n",
    "df_freq.select(\"filtered\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402e14f",
   "metadata": {},
   "source": [
    "再用 `IDF` 对词频进行规约化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "idf_model = idf.fit(df_freq)\n",
    "df_scaled = idf_model.transform(df_freq)\n",
    "df_scaled.select(\"content\", \"scaled_features\").show(n=15, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7924b2",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64324dea",
   "metadata": {},
   "source": [
    "我们利用得到的特征对弹幕进行 K-means 聚类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(10).setSeed(123)\n",
    "kmeans.setMaxIter(100)\n",
    "kmeans.setFeaturesCol(\"scaled_features\")\n",
    "\n",
    "kmeans_model = kmeans.fit(df_scaled)\n",
    "kmeans_model.setPredictionCol(\"cluster_labels\")\n",
    "\n",
    "df_pred = kmeans_model.transform(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783bc18",
   "metadata": {},
   "source": [
    "此时 `df_pred` 中包含了最终的聚类结果和若干中间变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09dd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f623b72",
   "metadata": {},
   "source": [
    "查看聚类结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15e6c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pred.select(\"content\", \"cluster_labels\").groupBy(\"cluster_labels\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78eeb52",
   "metadata": {},
   "source": [
    "最后将聚类结果与原始弹幕和剧集相对照："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = spark.read.json(\"data/lec14-video-data.json\", multiLine=True)\n",
    "video_title = video_info.select(\"cid\", \"title\")\n",
    "video_title.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_pred.select(\"cid\", \"content\", \"cluster_labels\").filter(\"cluster_labels > 0\")\n",
    "df_res.join(video_title, df_res.cid == video_title.cid, \"inner\").drop(df_res.cid).\\\n",
    "    drop(video_title.cid).sort(\"cluster_labels\").show(n=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
